{
  "results": {
    "arc_challenge": {
      "alias": "arc_challenge",
      "acc,none": 0.439419795221843,
      "acc_stderr,none": 0.014503747823580122,
      "acc_norm,none": 0.4453924914675768,
      "acc_norm_stderr,none": 0.014523987638344086
    },
    "arc_easy": {
      "alias": "arc_easy",
      "acc,none": 0.7605218855218855,
      "acc_stderr,none": 0.008757032594354022,
      "acc_norm,none": 0.73989898989899,
      "acc_norm_stderr,none": 0.009001718541079954
    },
    "gsm8k": {
      "alias": "gsm8k",
      "exact_match,strict-match": 0.0,
      "exact_match_stderr,strict-match": 0.0,
      "exact_match,flexible-extract": 0.05458680818802123,
      "exact_match_stderr,flexible-extract": 0.00625744403791253
    },
    "mmlu": {
      "acc,none": 0.4126904999287851,
      "acc_stderr,none": 0.004063155021833182,
      "alias": "mmlu"
    },
    "mmlu_humanities": {
      "acc,none": 0.3948990435706695,
      "acc_stderr,none": 0.006944389532868056,
      "alias": " - humanities"
    },
    "mmlu_formal_logic": {
      "alias": "  - formal_logic",
      "acc,none": 0.3492063492063492,
      "acc_stderr,none": 0.04263906892795133
    },
    "mmlu_high_school_european_history": {
      "alias": "  - high_school_european_history",
      "acc,none": 0.6,
      "acc_stderr,none": 0.03825460278380026
    },
    "mmlu_high_school_us_history": {
      "alias": "  - high_school_us_history",
      "acc,none": 0.5049019607843137,
      "acc_stderr,none": 0.035091433756067866
    },
    "mmlu_high_school_world_history": {
      "alias": "  - high_school_world_history",
      "acc,none": 0.5738396624472574,
      "acc_stderr,none": 0.03219035703131774
    },
    "mmlu_international_law": {
      "alias": "  - international_law",
      "acc,none": 0.5867768595041323,
      "acc_stderr,none": 0.04495087843548408
    },
    "mmlu_jurisprudence": {
      "alias": "  - jurisprudence",
      "acc,none": 0.48148148148148145,
      "acc_stderr,none": 0.04830366024635331
    },
    "mmlu_logical_fallacies": {
      "alias": "  - logical_fallacies",
      "acc,none": 0.4662576687116564,
      "acc_stderr,none": 0.03919415545048411
    },
    "mmlu_moral_disputes": {
      "alias": "  - moral_disputes",
      "acc,none": 0.3959537572254335,
      "acc_stderr,none": 0.026329813341946243
    },
    "mmlu_moral_scenarios": {
      "alias": "  - moral_scenarios",
      "acc,none": 0.28044692737430166,
      "acc_stderr,none": 0.01502408388332289
    },
    "mmlu_philosophy": {
      "alias": "  - philosophy",
      "acc,none": 0.4983922829581994,
      "acc_stderr,none": 0.02839794490780661
    },
    "mmlu_prehistory": {
      "alias": "  - prehistory",
      "acc,none": 0.44753086419753085,
      "acc_stderr,none": 0.02766713856942269
    },
    "mmlu_professional_law": {
      "alias": "  - professional_law",
      "acc,none": 0.31290743155149936,
      "acc_stderr,none": 0.011842529823062994
    },
    "mmlu_world_religions": {
      "alias": "  - world_religions",
      "acc,none": 0.6374269005847953,
      "acc_stderr,none": 0.03687130615562059
    },
    "mmlu_other": {
      "acc,none": 0.4628258770518185,
      "acc_stderr,none": 0.008836581961755938,
      "alias": " - other"
    },
    "mmlu_business_ethics": {
      "alias": "  - business_ethics",
      "acc,none": 0.46,
      "acc_stderr,none": 0.05009082659620332
    },
    "mmlu_clinical_knowledge": {
      "alias": "  - clinical_knowledge",
      "acc,none": 0.43018867924528303,
      "acc_stderr,none": 0.030471445867183235
    },
    "mmlu_college_medicine": {
      "alias": "  - college_medicine",
      "acc,none": 0.3930635838150289,
      "acc_stderr,none": 0.03724249595817729
    },
    "mmlu_global_facts": {
      "alias": "  - global_facts",
      "acc,none": 0.31,
      "acc_stderr,none": 0.04648231987117316
    },
    "mmlu_human_aging": {
      "alias": "  - human_aging",
      "acc,none": 0.45739910313901344,
      "acc_stderr,none": 0.033435777055830646
    },
    "mmlu_management": {
      "alias": "  - management",
      "acc,none": 0.4077669902912621,
      "acc_stderr,none": 0.04865777570410769
    },
    "mmlu_marketing": {
      "alias": "  - marketing",
      "acc,none": 0.5982905982905983,
      "acc_stderr,none": 0.03211693751051621
    },
    "mmlu_medical_genetics": {
      "alias": "  - medical_genetics",
      "acc,none": 0.43,
      "acc_stderr,none": 0.049756985195624284
    },
    "mmlu_miscellaneous": {
      "alias": "  - miscellaneous",
      "acc,none": 0.565772669220945,
      "acc_stderr,none": 0.017724589389677785
    },
    "mmlu_nutrition": {
      "alias": "  - nutrition",
      "acc,none": 0.4411764705882353,
      "acc_stderr,none": 0.028431095444176647
    },
    "mmlu_professional_accounting": {
      "alias": "  - professional_accounting",
      "acc,none": 0.3546099290780142,
      "acc_stderr,none": 0.02853865002887864
    },
    "mmlu_professional_medicine": {
      "alias": "  - professional_medicine",
      "acc,none": 0.39338235294117646,
      "acc_stderr,none": 0.029674288281311172
    },
    "mmlu_virology": {
      "alias": "  - virology",
      "acc,none": 0.4036144578313253,
      "acc_stderr,none": 0.03819486140758398
    },
    "mmlu_social_sciences": {
      "acc,none": 0.45921351966200846,
      "acc_stderr,none": 0.008830307727475156,
      "alias": " - social sciences"
    },
    "mmlu_econometrics": {
      "alias": "  - econometrics",
      "acc,none": 0.24561403508771928,
      "acc_stderr,none": 0.040493392977481425
    },
    "mmlu_high_school_geography": {
      "alias": "  - high_school_geography",
      "acc,none": 0.4595959595959596,
      "acc_stderr,none": 0.035507024651313425
    },
    "mmlu_high_school_government_and_politics": {
      "alias": "  - high_school_government_and_politics",
      "acc,none": 0.5284974093264249,
      "acc_stderr,none": 0.03602573571288442
    },
    "mmlu_high_school_macroeconomics": {
      "alias": "  - high_school_macroeconomics",
      "acc,none": 0.3384615384615385,
      "acc_stderr,none": 0.023991500500313036
    },
    "mmlu_high_school_microeconomics": {
      "alias": "  - high_school_microeconomics",
      "acc,none": 0.3865546218487395,
      "acc_stderr,none": 0.0316314580755238
    },
    "mmlu_high_school_psychology": {
      "alias": "  - high_school_psychology",
      "acc,none": 0.5211009174311927,
      "acc_stderr,none": 0.021418224754264643
    },
    "mmlu_human_sexuality": {
      "alias": "  - human_sexuality",
      "acc,none": 0.5114503816793893,
      "acc_stderr,none": 0.043841400240780176
    },
    "mmlu_professional_psychology": {
      "alias": "  - professional_psychology",
      "acc,none": 0.4117647058823529,
      "acc_stderr,none": 0.01991037746310594
    },
    "mmlu_public_relations": {
      "alias": "  - public_relations",
      "acc,none": 0.43636363636363634,
      "acc_stderr,none": 0.04750185058907297
    },
    "mmlu_security_studies": {
      "alias": "  - security_studies",
      "acc,none": 0.4897959183673469,
      "acc_stderr,none": 0.03200255347893782
    },
    "mmlu_sociology": {
      "alias": "  - sociology",
      "acc,none": 0.6417910447761194,
      "acc_stderr,none": 0.03390393042268814
    },
    "mmlu_us_foreign_policy": {
      "alias": "  - us_foreign_policy",
      "acc,none": 0.68,
      "acc_stderr,none": 0.04688261722621504
    },
    "mmlu_stem": {
      "acc,none": 0.34443387250237867,
      "acc_stderr,none": 0.008364917248161874,
      "alias": " - stem"
    },
    "mmlu_abstract_algebra": {
      "alias": "  - abstract_algebra",
      "acc,none": 0.22,
      "acc_stderr,none": 0.0416333199893227
    },
    "mmlu_anatomy": {
      "alias": "  - anatomy",
      "acc,none": 0.42962962962962964,
      "acc_stderr,none": 0.04276349494376599
    },
    "mmlu_astronomy": {
      "alias": "  - astronomy",
      "acc,none": 0.4342105263157895,
      "acc_stderr,none": 0.04033565667848319
    },
    "mmlu_college_biology": {
      "alias": "  - college_biology",
      "acc,none": 0.4375,
      "acc_stderr,none": 0.04148415739394154
    },
    "mmlu_college_chemistry": {
      "alias": "  - college_chemistry",
      "acc,none": 0.36,
      "acc_stderr,none": 0.048241815132442176
    },
    "mmlu_college_computer_science": {
      "alias": "  - college_computer_science",
      "acc,none": 0.34,
      "acc_stderr,none": 0.04760952285695236
    },
    "mmlu_college_mathematics": {
      "alias": "  - college_mathematics",
      "acc,none": 0.34,
      "acc_stderr,none": 0.04760952285695235
    },
    "mmlu_college_physics": {
      "alias": "  - college_physics",
      "acc,none": 0.22549019607843138,
      "acc_stderr,none": 0.041583075330832865
    },
    "mmlu_computer_security": {
      "alias": "  - computer_security",
      "acc,none": 0.5,
      "acc_stderr,none": 0.050251890762960605
    },
    "mmlu_conceptual_physics": {
      "alias": "  - conceptual_physics",
      "acc,none": 0.33617021276595743,
      "acc_stderr,none": 0.030881618520676942
    },
    "mmlu_electrical_engineering": {
      "alias": "  - electrical_engineering",
      "acc,none": 0.4,
      "acc_stderr,none": 0.04082482904638628
    },
    "mmlu_elementary_mathematics": {
      "alias": "  - elementary_mathematics",
      "acc,none": 0.24074074074074073,
      "acc_stderr,none": 0.022019080012217883
    },
    "mmlu_high_school_biology": {
      "alias": "  - high_school_biology",
      "acc,none": 0.45806451612903226,
      "acc_stderr,none": 0.02834378725054063
    },
    "mmlu_high_school_chemistry": {
      "alias": "  - high_school_chemistry",
      "acc,none": 0.3399014778325123,
      "acc_stderr,none": 0.033327690684107895
    },
    "mmlu_high_school_computer_science": {
      "alias": "  - high_school_computer_science",
      "acc,none": 0.37,
      "acc_stderr,none": 0.048523658709391
    },
    "mmlu_high_school_mathematics": {
      "alias": "  - high_school_mathematics",
      "acc,none": 0.25925925925925924,
      "acc_stderr,none": 0.026719240783712163
    },
    "mmlu_high_school_physics": {
      "alias": "  - high_school_physics",
      "acc,none": 0.271523178807947,
      "acc_stderr,none": 0.036313298039696525
    },
    "mmlu_high_school_statistics": {
      "alias": "  - high_school_statistics",
      "acc,none": 0.32407407407407407,
      "acc_stderr,none": 0.03191923445686186
    },
    "mmlu_machine_learning": {
      "alias": "  - machine_learning",
      "acc,none": 0.38392857142857145,
      "acc_stderr,none": 0.04616143075028547
    },
    "truthfulqa_mc2": {
      "alias": "truthfulqa_mc2",
      "acc,none": 0.40578370146582166,
      "acc_stderr,none": 0.01387933170065798
    }
  },
  "groups": {
    "mmlu": {
      "acc,none": 0.4126904999287851,
      "acc_stderr,none": 0.004063155021833182,
      "alias": "mmlu"
    },
    "mmlu_humanities": {
      "acc,none": 0.3948990435706695,
      "acc_stderr,none": 0.006944389532868056,
      "alias": " - humanities"
    },
    "mmlu_other": {
      "acc,none": 0.4628258770518185,
      "acc_stderr,none": 0.008836581961755938,
      "alias": " - other"
    },
    "mmlu_social_sciences": {
      "acc,none": 0.45921351966200846,
      "acc_stderr,none": 0.008830307727475156,
      "alias": " - social sciences"
    },
    "mmlu_stem": {
      "acc,none": 0.34443387250237867,
      "acc_stderr,none": 0.008364917248161874,
      "alias": " - stem"
    }
  }
}